---
title: "p8105_hw3_zw2899"
author: "Ziqing Wang"
date: "2022-10-10"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(patchwork)
library(ggridges)
library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# problem 1

First, immport the instacart dataset accoriding to the instructions:
```{r}
library(p8105.datasets)
data("instacart")
```

The dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. Each observation contains information on one product ordered by the user.

There are `r nrow(distinct(instacart, aisle))` distinct aisles. 

Below is a plot that shows the number of items ordered in aisles with more than 10000 items ordered, arranged by descending items ordered (from the provided solutions).


```{r ordered item count by aisle plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

Below is a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, along with the number of times each item is ordered.

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n_ordered = n()) %>%
  arrange(aisle, desc(n_ordered)) %>%
  group_by(aisle) %>%
  slice(1:3) %>%
  rename(most_ordered_product = product_name) 
  

```

Below is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week in a readable format.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  mutate(new_dow = order_dow + 1) %>%
  mutate(day_of_week = wday(new_dow, label = T)) %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour_ordered = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = day_of_week, values_from = mean_hour_ordered)
```



# problem 2

First, import and tidy the accelerometer data:

```{r}
accel_data = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  # convert the data to the longer format that is more suitable for manipulation using tidyverse
  pivot_longer(cols = "activity_1":"activity_1440", 
               names_to = "minute", 
               values_to = "activity_count") %>%
  # create a new variable indicataing whether the day is a weekday or a weekend
  mutate(if_weekend = case_when(day %in% c("Friday", "Thursday", "Wednesday", "Tuesday", "Monday") ~ "weekday",
                                TRUE ~ "weekend")) %>%
  # convert the activity_xx variable to a minute variable using separate()
  separate(minute, c("m1", "minute"), sep = "_") %>%
  select(-m1) %>% # remove the acticity prefix
  # convert the minute variable from character to numeric
  # create an hour variable indicating which hour of the day an observation is in
  mutate(minute = as.numeric(minute),
         hour = rep(rep(seq(1,24), each=60), 35)) %>%
  # convert the day variable to a factor with conventional level order
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))

accel_data
```

The tidied dataset contains `r nrow(accel_data)` observations and `r ncol(accel_data)` variables. The dataset contains minute-by-minute activity count of a person everyday for for `r max(accel_data$week)` weeks.  

Next, aggregate across minutes to create a total activity variable for each day:

```{r}
total_activity_summary = accel_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) 

total_activity_summary
```

From the summary table above, I don't see any apparent trend of activity count by day. We make some plots to verify this:

```{r activity count by day line plot}
total_activity_summary %>%
  ggplot(aes(x = week, y = total_activity, color = day, group = day)) +
  geom_point() +
  geom_line() + 
  labs(y = "total activity count by days in week", 
       title = "total activity count vs. week by days in week")
```
I don't see any apparent trend either from the plot above. 

Next, I aggregate the data by hour to make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r }
# prepare a dataframe for plotting
hourly_accel_data = accel_data %>%
  group_by(week, day, hour) %>%
  summarize(total_activity_hourly = sum(activity_count)) 
 
hourly_accel_data
```

```{r 24-hour activity plot}
hourly_accel_data %>% ggplot(aes(x = hour, y = total_activity_hourly, 
                                 group = interaction(week, day), color = day)) + 
  geom_point(alpha = 0.5) +
  xlim(1,24) +
  geom_smooth(se = F, size = 0.8, alpha = 0.8) +
  labs(title = "total activity count over hours for each day of week",
       y = "total hourly activity count") +
  scale_x_continuous(breaks = seq(1, 24, by = 1))
```
I overlayed lines fitted by geom_smooth() over the data points for more informative and aesthetic visualization than line plots. The plot shows that overall, the person's activity level increases from midnight to about 10am everyday. The activity mostly peaks aroound 7pm/8pm.   

# problem 3

First, load the NY NOAA data
```{r}
data("ny_noaa")
```

The imported NOAA dataset has `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` columns. The dataset contains information reported from weather stations in the state of New York on dates from January 1, 1981 through December 31, 2010 - including the precipitation (tenth of mm), snowfall (mm), snow depth (mm), maximum daily temperature, and minimum daily temperature (both in tenth of Celsius). Note that there are many missing data in this dataset:

```{r}
total_obs_noaa = nrow(ny_noaa)
ny_noaa %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(id:tmin, names_to = "variable", values_to = "percent_missing") %>%
  mutate(percent_missing = percent_missing/total_obs_noaa*100)

```
We can see that precipitation, snowfall, and small depth has relatively small number of missing values. However, almost half of the minimum and maximum temperature data are missing in this dataset, which might lead to bias in analyses.  

We clean the data by converting the unit of tmax and tmin from tenth of Celsius to Celsius. We also convert the unit of precipitation from tenth of mm to mm. We also separate the date variable to year, month, and day variables. We choose to not change the unit of the snowfall variable because mm is already a widely-used unit worldwide.
```{r}
cleaned_ny_noaa = ny_noaa %>%
  mutate(date_full = date) %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         prcp = prcp/10) # convert tenths of values to to actual values

cleaned_ny_noaa
```

We go through some extra steps to convert the year, month, and day variables to number, character, and number format:
```{r}
cleaned_ny_noaa = cleaned_ny_noaa %>%
  mutate(year = as.integer(year),
         month = month.name[as.integer(month)],
         day = as.integer(day))
cleaned_ny_noaa
```

The most commonly observed values in snowfall is calculated as follows:
```{r}
cleaned_ny_noaa %>%
  group_by(snow) %>%
  summarize(snowfall_mm_count = n()) %>%
  arrange(desc(snowfall_mm_count))
```
We can see from above that the most common values for snowfall is 0. This is probably because for most of the places in New York, it doesn't snow most time in the year.

Below is a plot showing the average maximum temperature in January and July in each weather station across years:
```{r average tmax in Jan and July across by weather station}
cleaned_ny_noaa %>%
  filter(month %in% c("January", "July")) %>%
  group_by(id, year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = T)) %>%
  filter(!is.na(mean_tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, col = month, group = id)) +
  geom_line(alpha = 0.3) +
  facet_grid(.~ month) + 
  theme(legend.position = "none") +
  labs(title = "average maximum temperature in January vs. July for each weather station",
       y = "average maximum temperature (Celsius)")
```
Based on the above 2-panel plot, it seems like the average maximum temperature in January and July in stations across years vary together in the same direction, i.e., higher maximum temperature in January corresponds to higher maximum temperature in July as well.  

One outlier is that, around 1987, the average maximum temperature in July of one station (~15 Celsius) was much lower than that for other stations (~30 Celsius).   

Another outlier us that, around 1982, the average maximum temperature in January of one station (< -10 Celsius) was much lower than that for other stations (mostly between 0 Celsius and -19 Celsius).   

Another outlier was that, around 2004, the average maximum temperature in January of one station (~10 Celsius) was much higher than that for other stations (mostly between 0 Celsius and -10 Celsius).    

Below is a plot showing tmax vs tmin for the full dataset. Alternative to a scatterplot, we plot tmin and tmax from different years against days of the year and observe how one changes with the other:  

```{r tmax-tmin association plot}
tmax_tmin_full_data = cleaned_ny_noaa %>%
  filter(!is.na(tmin) & !is.na(tmax)) %>%
  filter(tmin < 50 & tmax < 60) %>% # filter out unreasonable temperature extremes
  mutate(day_365 = yday(date_full)) %>%
  pivot_longer(cols = tmax:tmin, names_to = "extrema_type", values_to = "temp") %>%
  ggplot() +
  geom_line(aes(x = day_365, y = temp, color = extrema_type, group = interaction(year, extrema_type)),
            alpha = 0.05) +
  #scale_color_manual(name = "", values = c("red", "blue"), labels = c("tmax", "tmin")) +
  guides(colour = guide_legend(override.aes = list(alpha = 0.8)),
         fill=guide_legend(title="extrema type")) +
  labs(title = "minimum and maximum temperature over year in multiple years",
       x = "day of the year", y = "temperature (Celsius)") 
  #theme(legend.position = "bottom")
  #facet_grid(. ~ tmax_or_tmin) +
  #theme(legend.position = "none")
  #geom_smooth(se = F) 

tmax_tmin_full_data
```
We can see that in general, the maximum temperature and the minimum temperature vary together in the same direction.  

Below is a plot of the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r snowfall distribution plot, fig.height = 10}
snowfall_dist = cleaned_ny_noaa %>%
  mutate(year = factor(year)) %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = snow, y = year, fill = year)) +
  geom_density_ridges(alpha = 0.5, scale = 2)  +
  labs(title = "snowfall distribution (between 0 and 100mm) over years",
       x = "snowfall (mm)", y = "year") 

snowfall_dist
```
From the plot above, we can see that the distributions of snowfall values greater than 0 and less than 100 are pretty similar every year. 

Use the Patchwork package to make a two-panel plot from the two previous plots:
```{r 2-panel plot, fig.height = 12}
tmax_tmin_full_data / snowfall_dist + 
  plot_layout(byrow = F, heights = c(1, 3)) 
```



