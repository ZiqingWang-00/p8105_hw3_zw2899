---
title: "p8105_hw3_zw2899"
author: "Ziqing Wang"
date: "2022-10-10"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(lubridate)
```

# problem 1

First, immport the instacart dataset accoriding to the instructions:
```{r}
library(p8105.datasets)
data("instacart")
```

The dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. Each observation contains information on one product ordered by the user.

There are `r nrow(distinct(instacart, aisle))` distinct aisles. 

Below is a bar plot that shows the number of items ordered in aisles with more than 10000 items ordered, arranged by descending items ordered.


```{r fig.height = 30, fig.width = 15}
instacart %>%
  group_by(aisle) %>%
  summarize(n_ordered_by_aisle = n()) %>%
  filter(n_ordered_by_aisle >= 10000) %>%
  arrange(desc(n_ordered_by_aisle)) %>%
  mutate(order_magnitude = case_when(
    n_ordered_by_aisle <= 15000 ~ "<=15K",
    n_ordered_by_aisle <= 20000 & n_ordered_by_aisle > 15000 ~ "15K-20K",
    n_ordered_by_aisle <= 30000 & n_ordered_by_aisle > 20000 ~ "20K-30K",
    n_ordered_by_aisle <= 50000 & n_ordered_by_aisle > 30000 ~ "30K-50K",
    TRUE ~ ">50K"
    )) %>%
  mutate(order_magnitude = factor(order_magnitude, levels = c(">50K", "30K-50K", "20K-30K", "15K-20K", "<=15K"))) %>%
  ggplot(aes(x = aisle, y = n_ordered_by_aisle)) + 
  geom_col() + 
  facet_wrap(vars(order_magnitude), scales = "free", ncol = 1) + 
  labs(title = "number of orders on popular aisles", y = "number of orders") +
  theme_bw(base_size=50) +
  theme(panel.spacing.y = unit(10, "line"),
        axis.text.x = element_text(angle = 30, hjust=1))

```

Below is a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, along with the number of times each item is ordered.

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n_ordered = n()) %>%
  arrange(aisle, desc(n_ordered)) %>%
  group_by(aisle) %>%
  slice(1:3) %>%
  rename(most_ordered_product = product_name) 
  

```

Below is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week in a readable format.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  mutate(new_dow = order_dow + 1) %>%
  mutate(day_of_week = wday(new_dow, label = T)) %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour_ordered = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = day_of_week, values_from = mean_hour_ordered)
```



# problem 2

First, import and tidy the accelerometer data:

```{r}
accel_data = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(cols = "activity_1":"activity_1440", 
               names_to = "minute", 
               values_to = "activity_count") %>%
  mutate(if_weekend = case_when(day %in% c("Friday", "Thursday", "Wednesday", "Tuesday", "Monday") ~ "weekday",
                                TRUE ~ "weekend")) %>%
  separate(minute, c("m1", "minute"), sep = "_") %>%
  select(-m1) %>%
  mutate(minute = as.numeric(minute),
         hour = rep(rep(seq(1,24), each=60), 35)) %>%
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
         day = factor(day))

accel_data
```

Next, aggregate across minutes to create a total activity variable for each day:
```{r}
total_activity_summary = accel_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) 

total_activity_summary
```

There is no apparent trend of activity count by day. 

```{r }
hourly_accel_data = accel_data %>%
  #filter(day %in% c("Saturday", "Friday") & week %in% c(1,2,3)) %>%
  group_by(week, day, hour) %>%
  summarize(total_activity_min = sum(activity_count)) 
 
hourly_accel_data
```

```{r}
hourly_accel_data %>% ggplot(aes(x = hour, y = total_activity_min, 
                                 group = interaction(week, day), color = day)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(se = F)
  #geom_line() 
```


# problem 3

First, load the NY NOAA data
```{r}
data("ny_noaa")
```

The imported NOAA dataset has `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` columns. The dataset contains information reported from weather stations in the state of New York on dates from January 1, 1981 through December 31, 2010 - including the precipitation (tenth of mm), snowfall (mm), snow depth (mm), maximum daily temperature, and minimum daily temperature (both in tenth of Celsius). Note that there are many missing data in this dataset:

```{r}
total_obs_noaa = nrow(ny_noaa)
ny_noaa %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(id:tmin, names_to = "variable", values_to = "percent_missing") %>%
  mutate(percent_missing = percent_missing/total_obs_noaa*100)

```
We can see that precipitation, snowfall, and small depth has relatively small number of missing values. However, almost half of the minimum and maximum temperature data are missing in this dataset.  

* Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r}
#ny_noaa %>%
#  separate(date, )
```










