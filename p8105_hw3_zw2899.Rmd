---
title: "p8105_hw3_zw2899"
author: "Ziqing Wang"
date: "2022-10-10"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(lubridate)
```

# problem 1

First, immport the instacart dataset accoriding to the instructions:
```{r}
library(p8105.datasets)
data("instacart")
```

The dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. Each observation contains information on one product ordered by the user.

There are `r nrow(distinct(instacart, aisle))` distinct aisles. 

Below is a bar plot that shows the number of items ordered in aisles with more than 10000 items ordered, arranged by descending items ordered.


```{r fig.height = 30, fig.width = 15}
instacart %>%
  group_by(aisle) %>%
  summarize(n_ordered_by_aisle = n()) %>%
  filter(n_ordered_by_aisle >= 10000) %>%
  arrange(desc(n_ordered_by_aisle)) %>%
  mutate(order_magnitude = case_when(
    n_ordered_by_aisle <= 15000 ~ "<=15K",
    n_ordered_by_aisle <= 20000 & n_ordered_by_aisle > 15000 ~ "15K-20K",
    n_ordered_by_aisle <= 30000 & n_ordered_by_aisle > 20000 ~ "20K-30K",
    n_ordered_by_aisle <= 50000 & n_ordered_by_aisle > 30000 ~ "30K-50K",
    TRUE ~ ">50K"
    )) %>%
  mutate(order_magnitude = factor(order_magnitude, levels = c(">50K", "30K-50K", "20K-30K", "15K-20K", "<=15K"))) %>%
  ggplot(aes(x = aisle, y = n_ordered_by_aisle)) + 
  geom_col() + 
  facet_wrap(vars(order_magnitude), scales = "free", ncol = 1) + 
  labs(title = "number of orders on popular aisles", y = "number of orders") +
  theme_bw(base_size=50) +
  theme(panel.spacing.y = unit(10, "line"),
        axis.text.x = element_text(angle = 30, hjust=1))

```

Below is a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”, along with the number of times each item is ordered.

```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>%
  summarize(n_ordered = n()) %>%
  arrange(aisle, desc(n_ordered)) %>%
  group_by(aisle) %>%
  slice(1:3) %>%
  rename(most_ordered_product = product_name) 
  

```

Below is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week in a readable format.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  mutate(new_dow = order_dow + 1) %>%
  mutate(day_of_week = wday(new_dow, label = T)) %>%
  group_by(product_name, day_of_week) %>%
  summarize(mean_hour_ordered = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = day_of_week, values_from = mean_hour_ordered)
```



# problem 2

First, import and tidy the accelerometer data:

```{r}
accel_data = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(cols = "activity_1":"activity_1440", 
               names_to = "minute", 
               values_to = "activity_count") %>%
  mutate(if_weekend = case_when(day %in% c("Friday", "Thursday", "Wednesday", "Tuesday", "Monday") ~ "weekday",
                                TRUE ~ "weekend")) %>%
  separate(minute, c("m1", "minute"), sep = "_") %>%
  select(-m1) %>%
  mutate(minute = as.numeric(minute),
         hour = rep(rep(seq(1,24), each=60), 35)) %>%
  mutate(day = factor(day, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
         day = factor(day))

accel_data
```

Next, aggregate across minutes to create a total activity variable for each day:
```{r}
total_activity_summary = accel_data %>%
  group_by(week, day) %>%
  summarize(total_activity = sum(activity_count)) 

total_activity_summary
```

There is no apparent trend of activity count by day. 

```{r }
hourly_accel_data = accel_data %>%
  group_by(week, day, hour) %>%
  summarize(total_activity_min = sum(activity_count)) 
 
hourly_accel_data
```

```{r}
hourly_accel_data %>% ggplot(aes(x = hour, y = total_activity_min, 
                                 group = interaction(week, day), color = day)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(se = F)
  #geom_line() 
```


# problem 3

First, load the NY NOAA data
```{r}
data("ny_noaa")
```

The imported NOAA dataset has `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` columns. The dataset contains information reported from weather stations in the state of New York on dates from January 1, 1981 through December 31, 2010 - including the precipitation (tenth of mm), snowfall (mm), snow depth (mm), maximum daily temperature, and minimum daily temperature (both in tenth of Celsius). Note that there are many missing data in this dataset:

```{r}
total_obs_noaa = nrow(ny_noaa)
ny_noaa %>%
  summarise_all(funs(sum(is.na(.)))) %>%
  pivot_longer(id:tmin, names_to = "variable", values_to = "percent_missing") %>%
  mutate(percent_missing = percent_missing/total_obs_noaa*100)

```
We can see that precipitation, snowfall, and small depth has relatively small number of missing values. However, almost half of the minimum and maximum temperature data are missing in this dataset, which might lead to bias in analyses.  

We clean the data by converting the unit of tmax and tmin from tenth of Celsius to Celsius. We also convert the unit of precipitation from tenth of mm to mm. We also separate the date variable to year, month, and day variables. We choose to not change the unit of the snowfall variable because mm is already a widely-used unit worldwide.
```{r}
cleaned_ny_noaa = ny_noaa %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10,
         prcp = prcp/10) # convert tenths of values to to actual values

cleaned_ny_noaa
```

We go through some extra steps to convert the year, month, and day variables to number, character, and number format:
```{r}
cleaned_ny_noaa = cleaned_ny_noaa %>%
  mutate(year = as.integer(year),
         month = month.name[as.integer(month)],
         day = as.integer(day))
cleaned_ny_noaa
```


The most commonly observed values in snowfall is calculated as follows:
```{r}
cleaned_ny_noaa %>%
  group_by(snow) %>%
  summarize(snowfall_mm = n()) %>%
  arrange(desc(snowfall_mm))
```
We can see from above that the most common values for snowfall is 0. This is probably because for most of the places in New York, it doesn't snow most time of the year.

Below is a plot showing the average maximum temperature in January and July in each weather station across years:
```{r}
cleaned_ny_noaa %>%
  filter(month %in% c("January", "July")) %>%
  group_by(id, year, month) %>%
  summarize(mean_tmax = mean(tmax, na.rm = T)) %>%
  filter(!is.na(mean_tmax)) %>%
  ggplot(aes(x = year, y = mean_tmax, col = month, group = id)) +
  geom_line(alpha = 0.3) +
  facet_grid(.~ month) + 
  theme(legend.position = "none")
```

Below is a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}

```








